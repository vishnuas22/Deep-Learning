{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of reducing the learning rate in steps (Step Decay), Exponential Decay smoothly decreases the learning rate at each step using an exponential function:\n",
    "\n",
    "η t=η 0⋅e −kt\n",
    " \n",
    "Where:\n",
    "ηt is the learning rate at step \n",
    "\n",
    "η 0 is the initial learning rate.\n",
    "\n",
    "k is a decay rate (a small constant).\n",
    "\n",
    "e is Euler’s number (~2.718).\n",
    "\n",
    "t is the current training step/epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is Exponential Decay Useful?\n",
    "\n",
    "✅ Smooth Reduction – Unlike Step Decay, which makes sudden jumps, Exponential Decay ensures a gradual decrease, preventing instability.\n",
    "\n",
    "✅ Better Convergence – Keeps learning fast in the beginning and slows down at later stages.\n",
    "\n",
    "✅ Common in Deep Learning – Used in models like CNNs, RNNs, Transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real-World Analogy\n",
    "\n",
    "Think of boiling water and cooling it down.\n",
    "\n",
    "Initially, the temperature is high (fast learning).\n",
    "\n",
    "You gradually reduce the heat instead of turning it off suddenly (smooth decay).\n",
    "\n",
    "This helps stabilize the process without shocking the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dummy model\n",
    "\n",
    "model = torch.nn.Linear(2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set an initial learning rate\n",
    "\n",
    "initial_lr = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizer\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=initial_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Exponential LR Scheduler\n",
    "\n",
    "gamma = 0.9 # Decay factor\n",
    "\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1 , Learning rate : 0.09000\n",
      "Epoch : 2 , Learning rate : 0.08100\n",
      "Epoch : 3 , Learning rate : 0.07290\n",
      "Epoch : 4 , Learning rate : 0.06561\n",
      "Epoch : 5 , Learning rate : 0.05905\n",
      "Epoch : 6 , Learning rate : 0.05314\n",
      "Epoch : 7 , Learning rate : 0.04783\n",
      "Epoch : 8 , Learning rate : 0.04305\n",
      "Epoch : 9 , Learning rate : 0.03874\n",
      "Epoch : 10 , Learning rate : 0.03487\n"
     ]
    }
   ],
   "source": [
    "# Simulate learning rate decay for 10 epochs\n",
    "\n",
    "for epoch in range(10):\n",
    "\n",
    "    optimizer.step()  # Simulating an optimizer step\n",
    "\n",
    "    scheduler.step()  # Simulating an optimizer step\n",
    "\n",
    "    print(f'Epoch : {epoch +1} , Learning rate : {scheduler.get_last_lr()[0]:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a linear model (dummy).\n",
    "\n",
    "Use SGD with an initial learning rate of 0.1.\n",
    "\n",
    "Apply Exponential Decay using gamma=0.9 (meaning LR reduces by 10% every step).\n",
    "\n",
    "Print the learning rate at each epoch."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterxdl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
