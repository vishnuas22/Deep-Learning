{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why Do We Need Hidden Layers?\n",
    "\n",
    "In the XOR problem, a simple linear model (without hidden layers) can’t solve the problem correctly, because XOR is a non-linear function. \n",
    "\n",
    "However, when we introduce hidden layers, we give the model more capacity to learn complex, non-linear patterns. \n",
    "\n",
    "Multi-layer Perceptrons (MLPs) can learn non-linear decision boundaries because of these hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structure of the Model:\n",
    "\n",
    "Input Layer: Takes 2 features (X1 and X2) from the dataset.\n",
    "\n",
    "Hidden Layer: A layer that applies non-linear transformations to the input. We’ll use an activation function like ReLU or Sigmoid here.\n",
    "\n",
    "Output Layer: Produces a binary output (0 or 1 for XOR)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "\n",
    "Define a Neural Network: With one hidden layer.\n",
    "\n",
    "Use Backpropagation: Update weights using gradients.\n",
    "\n",
    "Train the Model: Use MSE Loss for the XOR dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create XOR Dataset\n",
    "\n",
    "X = torch.tensor([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]], dtype=torch.float32)\n",
    "\n",
    "y = torch.tensor([[0.0], [1.0], [1.0], [0.0]], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define a simple Neural Network with 1 hidden layer\n",
    "\n",
    "class XORNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(XORNetwork,self).__init__()\n",
    "\n",
    "        # 2 input features -> 4 neurons in hidden layer -> 1 output\n",
    "\n",
    "        self.fc1 = nn.Linear(2, 4)  # Input layer (2 features) to hidden layer (4 neurons)\n",
    "\n",
    "        self.fc2 = nn.Linear(4, 1)  # Hidden layer (4 neurons) to output layer (1 neuron)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid() # Activation function for output layer\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        x = torch.relu(self.fc1(x)) # ReLU activation for hidden layer\n",
    "\n",
    "        x = self.sigmoid(self.fc2(x)) # Sigmoid activation for output\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Instantiate the model\n",
    "\n",
    "model = XORNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Define a loss function and an optimizer\n",
    "\n",
    "loss_fn = nn.MSELoss() # Mean Squared Error loss\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000, Loss: 0.2650403082370758\n",
      "Epoch 1001/10000, Loss: 0.18173818290233612\n",
      "Epoch 2001/10000, Loss: 0.16746756434440613\n",
      "Epoch 3001/10000, Loss: 0.1669771671295166\n",
      "Epoch 4001/10000, Loss: 0.16685958206653595\n",
      "Epoch 5001/10000, Loss: 0.16679951548576355\n",
      "Epoch 6001/10000, Loss: 0.16676753759384155\n",
      "Epoch 7001/10000, Loss: 0.166743665933609\n",
      "Epoch 8001/10000, Loss: 0.16673122346401215\n",
      "Epoch 9001/10000, Loss: 0.16672486066818237\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Train the model using backpropagation\n",
    "\n",
    "epochs = 10000 # Number of training iterations\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "        # Forward pass: Compute predicted y by passing X to the model\n",
    "\n",
    "        predictions = model(X)\n",
    "\n",
    "        # Compute the loss (error)\n",
    "\n",
    "        loss = loss_fn(predictions, y)\n",
    "\n",
    "        # Zero the gradients before backward pass\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Backward pass: Compute gradients\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights and biases using gradient descent\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print loss every 1000 epochs to monitor progress\n",
    "\n",
    "        if epoch % 1000 == 0:\n",
    "                \n",
    "                    print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned Weights (Input → Hidden Layer):\n",
      "tensor([[ 0.2998,  0.2307],\n",
      "        [-0.6119, -0.6878],\n",
      "        [-0.5591, -0.4051],\n",
      "        [-1.9148,  1.9110]])\n",
      "Learned Bias (Hidden Layer):\n",
      "tensor([-0.5309, -0.3873, -0.2462, -0.0032])\n",
      "Learned Weights (Hidden → Output Layer):\n",
      "tensor([[0.0307, 0.3604, 0.0539, 2.6001]])\n",
      "Learned Bias (Output Layer):\n",
      "tensor([-0.6918])\n"
     ]
    }
   ],
   "source": [
    "# Inspect learned weights and bias for each layer\n",
    "\n",
    "learned_weights_fc1 = model.fc1.weight.data\n",
    "learned_bias_fc1 = model.fc1.bias.data\n",
    "\n",
    "learned_weights_fc2 = model.fc2.weight.data\n",
    "learned_bias_fc2 = model.fc2.bias.data\n",
    "\n",
    "print(f\"Learned Weights (Input → Hidden Layer):\\n{learned_weights_fc1}\")\n",
    "print(f\"Learned Bias (Hidden Layer):\\n{learned_bias_fc1}\")\n",
    "\n",
    "print(f\"Learned Weights (Hidden → Output Layer):\\n{learned_weights_fc2}\")\n",
    "print(f\"Learned Bias (Output Layer):\\n{learned_bias_fc2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Predctions \n",
      "\n",
      "tensor([[0.3336],\n",
      "        [0.9862],\n",
      "        [0.3336],\n",
      "        [0.3336]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Final predictions after training\n",
    "\n",
    "with torch.no_grad(): # Disable gradient computation for inference\n",
    "\n",
    "    final_predictions = model(X)\n",
    "\n",
    "    print(f'Final Predctions \\n')\n",
    "\n",
    "    print(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterxdl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
