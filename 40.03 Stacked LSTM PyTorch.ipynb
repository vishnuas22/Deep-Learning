{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80841a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn as nn \n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b5455ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample corpus\n",
    "\n",
    "sentences = [\n",
    "    \"deep learning is powerful\",\n",
    "    \"deep learning is fun\",\n",
    "    \"learning is fun and powerful\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4343d7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "\n",
    "words = list(set(\" \".join(sentences).split()))\n",
    "\n",
    "word_to_idx = {word: idx for idx, word  in enumerate(words)}\n",
    "\n",
    "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "\n",
    "vocab_size = len(word_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ad01defb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data (3-word input â†’ 1-word output)\n",
    "\n",
    "def make_sequences(sentences, context_size=3):\n",
    "\n",
    "    input_seqs, target_words = [], []\n",
    "\n",
    "    for sentence in sentences:\n",
    "\n",
    "        tokens = sentence.split()\n",
    "\n",
    "        if len(tokens) >= context_size + 1:\n",
    "\n",
    "            for i in range(len(tokens) - context_size):\n",
    "\n",
    "                context = tokens[i:context_size]\n",
    "\n",
    "                target = tokens[i + context_size]\n",
    "\n",
    "                input_seqs.append(torch.tensor([word_to_idx[w] for w in context]))\n",
    "\n",
    "                target_words.append(torch.tensor(word_to_idx[target]))\n",
    "\n",
    "                return input_seqs, target_words\n",
    "            \n",
    "input_seqs, target_words = make_sequences(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a405d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacked LSTM model\n",
    "\n",
    "class StackedLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers = 2 ):\n",
    "        super(StackedLSTM, self).__init__()\n",
    "\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=num_layers)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "    \n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        x = x.permute(1, 0, 2)  # [seq_len, batch, embed_dim]\n",
    "\n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "\n",
    "        out = self.fc(out[-1])  # last time step\n",
    "\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "\n",
    "            # Initialize hidden state for all layers\n",
    "\n",
    "            return (torch.zeros(self.num_layers, 1, self.hidden_dim),\n",
    "                    torch.zeros(self.num_layers, 1, self.hidden_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f02e2f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparams\n",
    "\n",
    "embedding_dim = 10\n",
    "\n",
    "hidden_dim = 16\n",
    "\n",
    "num_layers = 2\n",
    "\n",
    "model = StackedLSTM(vocab_size, embedding_dim, hidden_dim, num_layers)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "90588e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/300, Loss: 0.0014\n",
      "Epoch 100/300, Loss: 0.0007\n",
      "Epoch 150/300, Loss: 0.0005\n",
      "Epoch 200/300, Loss: 0.0003\n",
      "Epoch 250/300, Loss: 0.0003\n",
      "Epoch 300/300, Loss: 0.0002\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "for epoch in range(1, 301):\n",
    "\n",
    "    total_loss = 0.0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for inp, target in zip(input_seqs, target_words):\n",
    "\n",
    "        inp = inp.unsqueeze(0)\n",
    "\n",
    "        target = target.unsqueeze(0)\n",
    "\n",
    "        hidden = model.init_hidden()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output, _ = model(inp, hidden)\n",
    "\n",
    "        loss = loss_fn(output, target)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    if epoch % 50 == 0:\n",
    "\n",
    "            print(f\"Epoch {epoch}/300, Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "54a27d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text generation (greedy)\n",
    "\n",
    "def generate_text(model, seed_words, length=5):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    generated = seed_words[:]\n",
    "\n",
    "    hidden = model.init_hidden()\n",
    "\n",
    "    for _ in range(length):\n",
    "\n",
    "        input_seq = [word_to_idx[w] for w in generated[-3:]]\n",
    "\n",
    "        input_tensor = torch.tensor(input_seq).unsqueeze(0)\n",
    "\n",
    "        output, hidden = model(input_tensor, hidden)\n",
    "\n",
    "        next_idx = torch.argmax(output, dim=1).item()\n",
    "\n",
    "        generated.append(idx_to_word[next_idx])\n",
    "\n",
    "    return \" \".join(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7fb7d8d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated:\n",
      "deep learning is powerful powerful powerful powerful powerful\n"
     ]
    }
   ],
   "source": [
    "# Test generation\n",
    "\n",
    "print(\"\\nGenerated:\")\n",
    "\n",
    "print(generate_text(model, ['deep', 'learning', 'is']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterxdl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
