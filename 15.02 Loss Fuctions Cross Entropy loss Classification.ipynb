{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-Entropy Loss is used for classification problems where we predict probabilities for multiple classes. \n",
    "It measures how different the predicted probability distribution is from the actual distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why Do We Need Cross-Entropy?\n",
    "\n",
    "Unlike MSE (which is good for regression), cross-entropy is designed for probability-based predictions.\n",
    "\n",
    "It strongly penalizes incorrect confident predictions, ensuring the model doesn’t become too confident in wrong predictions.\n",
    "\n",
    "Works best when combined with Softmax Activation in multi-class classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For binary classification (2 classes):\n",
    "\n",
    "L=−(ylog(ŷ)+(1−y)log(1− ŷ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For multi-class classification (more than 2 classes):\n",
    "\n",
    "L=− i=1∑C yi log( ŷi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where:\n",
    "\n",
    "y is the true label (one-hot encoded for multi-class).\n",
    "\n",
    "ŷ is the predicted probability for each class.\n",
    "\n",
    "C is the number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample logits (before softmax)\n",
    "\n",
    "logits = torch.tensor([[2.0, 1.0, 0.1], # Example 1\n",
    "                       [0.5, 2.5, 1.2],  # Example 2\n",
    "                       [1.0, 0.3, 3.1]]) # Example 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True labels (class indices)\n",
    "\n",
    "true_labels = torch.tensor([0,1,2]) # Correct classes for each example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Cross-Entropy Loss function\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Entropy Loss: 0.3091\n"
     ]
    }
   ],
   "source": [
    "# Compute loss\n",
    "\n",
    "loss = criterion(logits, true_labels)\n",
    "\n",
    "print(f\"Cross-Entropy Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "Logits: These are raw scores before applying softmax (3 classes per sample).\n",
    "\n",
    "True Labels: Given as class indices (not one-hot encoded).\n",
    "\n",
    "CrossEntropyLoss:\n",
    "\n",
    "Applies softmax internally.\n",
    "\n",
    "Converts logits into probabilities.\n",
    "\n",
    "Computes the negative log-likelihood for the correct class.\n",
    "\n",
    "Output: The final loss value."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterxdl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
