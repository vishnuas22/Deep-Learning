{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âœ… Manually define forward propagation\n",
    "\n",
    "âœ… Compute loss\n",
    "\n",
    "âœ… Manually compute gradients (without autograd)\n",
    "\n",
    "âœ… Update weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ”¹ Step 1: Initialize Data & Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input (X1, X2) and True Output (y)\n",
    "\n",
    "X = torch.tensor([0.5, 3.0])\n",
    "\n",
    "y_true = torch.tensor([1.0]) # True label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Weights and Bias\n",
    "\n",
    "W = torch.tensor([0.5, -1.2], dtype=torch.float32) # Two weights\n",
    "\n",
    "b = torch.tensor([0.7], dtype =torch.float32) # Bias\n",
    "\n",
    "# Learning Rate\n",
    "\n",
    "lr = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Forward Pass (Compute Output)\n",
    "\n",
    "We compute:\n",
    "z = W1.X1 + W2.X2 + b\n",
    "\n",
    "Then, apply the Sigmoid activation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted sum (z) : -2.6500000953674316\n",
      "Predicted Output (Å·) : 0.06598900258541107\n"
     ]
    }
   ],
   "source": [
    "# Compute weighted sum (z)\n",
    "\n",
    "z = W[0] * X[0] + W[1] * X[1] + b\n",
    "\n",
    "# Sigmoid Activation Function\n",
    "\n",
    "y_pred = 1 / (1 + torch.exp(-z))\n",
    "\n",
    "print(f'Weighted sum (z) : {z.item()}')\n",
    "\n",
    "print(f'Predicted Output (Å·) : {y_pred.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Compute Loss\n",
    "\n",
    "We use Mean Squared Error (MSE):\n",
    "\n",
    "     Loss = (y-Å·)^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 0.8723765015602112\n"
     ]
    }
   ],
   "source": [
    "# Compute Loss\n",
    "\n",
    "loss = (y_true - y_pred) **2\n",
    "\n",
    "print(f'Loss : {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Compute Gradients (Manual Backpropagation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient w.r.t W1: -0.057567257434129715\n",
      "Gradient w.r.t W2: -0.3454035520553589\n",
      "Gradient w.r.t Bias: -0.11513451486825943\n"
     ]
    }
   ],
   "source": [
    "# Compute gradients\n",
    "\n",
    "dL_dyhat = 2 * (y_pred - y_true) # dL/dÅ·\n",
    "\n",
    "dyhat_dz = y_pred * (1 - y_pred) # dÅ·/dz\n",
    "\n",
    "\n",
    "# Compute gradients for weights\n",
    "\n",
    "dL_dW1 = dL_dyhat * dyhat_dz *X[0]\n",
    "\n",
    "dL_dW2 = dL_dyhat * dyhat_dz *X[1]\n",
    "\n",
    "# Compute gradient for bias\n",
    "\n",
    "dL_db = dL_dyhat * dyhat_dz\n",
    "\n",
    "print(f\"Gradient w.r.t W1: {dL_dW1.item()}\")\n",
    "\n",
    "print(f\"Gradient w.r.t W2: {dL_dW2.item()}\")\n",
    "\n",
    "print(f\"Gradient w.r.t Bias: {dL_db.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Update Weights\n",
    "\n",
    "Using Gradient Descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Updated Weights and Bias:\n",
      "W1: 0.5057567358016968, W2: -1.1654596328735352, Bias: 0.7115134596824646\n"
     ]
    }
   ],
   "source": [
    "# Update weights using gradient descent\n",
    "\n",
    "W[0] -= lr * dL_dW1.item()\n",
    "\n",
    "W[1] -= lr * dL_dW2.item()\n",
    "\n",
    "b -= lr * dL_db.item()\n",
    "\n",
    "print(\"\\nUpdated Weights and Bias:\")\n",
    "\n",
    "print(f\"W1: {W[0].item()}, W2: {W[1].item()}, Bias: {b.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterxdl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
