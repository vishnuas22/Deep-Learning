{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d592e2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d73c13bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x111d077f0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set seed for reproducibility\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "44a11454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the character set\n",
    "\n",
    "chars = list(set('hello'))\n",
    "\n",
    "char2idx = {ch: idx for idx, ch in enumerate(chars)}\n",
    "\n",
    "idx2char = {idx: ch for ch, idx in char2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b49e447d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input and target\n",
    "\n",
    "input_str = 'hell'\n",
    "\n",
    "target_str = 'ello'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2f818de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input indices: tensor([0, 3, 2, 2])\n",
      "Target indices: tensor([3, 2, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "# Convert characters to indices\n",
    "\n",
    "input_seq = torch.tensor([char2idx[ch] for ch in input_str])\n",
    "\n",
    "\n",
    "target_seq = torch.tensor([char2idx[ch] for ch in target_str])\n",
    "\n",
    "\n",
    "print(\"Input indices:\", input_seq)\n",
    "\n",
    "print(\"Target indices:\", target_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0262d019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot input shape: torch.Size([4, 1, 4])\n",
      "One-hot encoded input:\n",
      " tensor([[[1., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 1.]],\n",
      "\n",
      "        [[0., 0., 1., 0.]],\n",
      "\n",
      "        [[0., 0., 1., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "# One-hot encoding function\n",
    "\n",
    "def one_hot_encode(seq, vocab_size):\n",
    "\n",
    "    one_hot = torch.zeros(len(seq), vocab_size)\n",
    "\n",
    "    for i , idx in enumerate(seq):\n",
    "\n",
    "        one_hot[i][idx] = 1.0\n",
    "\n",
    "    return one_hot\n",
    "\n",
    "vocab_size = len(chars) # number of unique characters\n",
    "\n",
    "# One-hot encode input\n",
    "\n",
    "input_one_hot = one_hot_encode(input_seq, vocab_size)\n",
    "\n",
    "# Reshape for RNN input: (sequence_len, batch_size, input_size)\n",
    "\n",
    "input_one_hot = input_one_hot.unsqueeze(1)\n",
    "\n",
    "\n",
    "print(\"One-hot input shape:\", input_one_hot.shape)  # [seq_len, batch, input_size]\n",
    "print(\"One-hot encoded input:\\n\", input_one_hot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1d1668c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaRNNCell(nn.Module):\n",
    "\n",
    "    def __init__(self,input_size,hidden_size, output_size):\n",
    "        super(VanillaRNNCell,self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Weight matrices for input and hidden state\n",
    "\n",
    "        self.i2h = nn.Linear(input_size + hidden_size , hidden_size)\n",
    "\n",
    "        self.h2o = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    \n",
    "    def forward(self, input_t, hidden):\n",
    "\n",
    "        combined = torch.cat((input_t, hidden),1)\n",
    "\n",
    "        hidden = torch.tanh(self.i2h(combined))\n",
    "\n",
    "        output = self.h2o(hidden)\n",
    "\n",
    "        output = self.softmax(output)\n",
    "\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "\n",
    "        return torch.zeros(1, self.hidden_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3b900d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step0 output : tensor([[-1.4189, -1.6050, -1.1916, -1.3728]], grad_fn=<LogSoftmaxBackward0>)\n",
      "Step1 output : tensor([[-1.6854, -1.5395, -1.0498, -1.3857]], grad_fn=<LogSoftmaxBackward0>)\n",
      "Step2 output : tensor([[-1.5752, -1.6865, -1.0236, -1.3920]], grad_fn=<LogSoftmaxBackward0>)\n",
      "Step3 output : tensor([[-1.5379, -1.6941, -1.0208, -1.4226]], grad_fn=<LogSoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "n_hidden = 8\n",
    "\n",
    "rnn = VanillaRNNCell(input_size=vocab_size, hidden_size=n_hidden, output_size=vocab_size)\n",
    "\n",
    "# Initialize hidden state\n",
    "\n",
    "hidden = rnn.init_hidden()\n",
    "\n",
    "# Forward through the sequence\n",
    "\n",
    "for i in range(input_one_hot.size(0)):\n",
    "\n",
    "    output, hidden = rnn(input_one_hot[i], hidden)\n",
    "\n",
    "    print(f'Step{i} output : {output}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "71213e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate the vocabulary mapping\n",
    "\n",
    "char_to_idx = {'h': 0, 'e': 1, 'l': 2, 'o': 3}\n",
    "\n",
    "idx_to_char = {v: k for k, v in char_to_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7dbed32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Indices: [2, 2, 2, 2]\n",
      "Predicted characters: ['l', 'l', 'l', 'l']\n"
     ]
    }
   ],
   "source": [
    "predicted_indices = []\n",
    "\n",
    "for i in range(input_one_hot.size(0)):\n",
    "\n",
    "    output, hidden = rnn(input_one_hot[i], hidden)\n",
    "\n",
    "    predicted_idx = torch.argmax(output, dim=1)\n",
    "\n",
    "    predicted_indices.append(predicted_idx.item())\n",
    "\n",
    "print('Predicted Indices:', predicted_indices)\n",
    "\n",
    "# Mapping back to characters\n",
    "\n",
    "idx_to_char = {v: k for k, v in char_to_idx.items()}\n",
    "\n",
    "predicted_chars = [idx_to_char[i] for i in predicted_indices]\n",
    "\n",
    "print(\"Predicted characters:\", predicted_chars)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eac52893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target_indices manually (based on your earlier tensor)\n",
    "\n",
    "target_indices = torch.tensor([1, 0, 0, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "41911ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target:e â†’ Predicted: l\n",
      "Target:h â†’ Predicted: l\n",
      "Target:h â†’ Predicted: l\n",
      "Target:o â†’ Predicted: l\n"
     ]
    }
   ],
   "source": [
    "# Convert target indices (tensors) to characters\n",
    "\n",
    "target_chars = [idx_to_char[i.item()] for i in target_indices] # target_indices is still a tensor\n",
    "\n",
    "# predicted_indices is a list of ints, so use them directly\n",
    "\n",
    "predicted_chars = [idx_to_char[i] for i in predicted_indices]\n",
    "\n",
    "# Print the comparison\n",
    "\n",
    "for t, p in zip(target_chars, predicted_chars):\n",
    "\n",
    "    print(f'Target:{t} â†’ Predicted: {p}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4267ab27",
   "metadata": {},
   "source": [
    "ðŸ§  Interpretation\n",
    "\n",
    " 1. Overfitting to a Single Class\n",
    "The model is likely predicting the most frequent class or falling into a pattern because:\n",
    "The input sequence is very short.\n",
    "There's no training (you're just passing it through once, not optimizing loss).\n",
    "You're using fixed weights initialized randomly â€” not trained.\n",
    "\n",
    " 2. This is an Inference Only Example\n",
    "You're doing manual inference without training â€” just forward passes with fixed weights, so:\n",
    "The model has no learned understanding of sequence dependencies yet.\n",
    "Predicting the same token ('C') is common in untrained or underfitted RNNs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterxdl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
