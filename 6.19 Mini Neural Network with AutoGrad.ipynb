{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create tensors with requires_grad=True (to track gradients)\n",
    "\n",
    "x = torch.tensor(3.0 , requires_grad=True) # Input\n",
    "\n",
    "w = torch.tensor(2.0, requires_grad=True) # Weight\n",
    "\n",
    "b = torch.tensor(1.0, requires_grad=True) # Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Compute the forward pass (Linear function: y = w*x + b)\n",
    "\n",
    "y = w * x + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Compute gradients using backpropagation\n",
    "\n",
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Print the gradients\n",
    "\n",
    "print('Gradient of y w.r.t x:', x.grad)  # Will be None (x is not a parameter)\n",
    "\n",
    "print('Gradient of y w.r.t w:',w.grad) # dy / dw = x\n",
    "\n",
    "print('Gradient of y w.r.t b:',b.grad) # dy / db = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âœ… How the Gradients are Computed?\n",
    "\n",
    "Using basic calculus:\n",
    "\n",
    "ðŸ”¹ dy/dw =  x = 3\n",
    "\n",
    "ðŸ”¹ dy / db = 1\n",
    "\n",
    "ðŸ”¹ dy / dx  is not needed because x is an input(not a learnable parameter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸŽ¯ Why is this Important for Deep Learning?\n",
    "\n",
    "In Neural Networks, weights & biases are updated using these gradients during training.\n",
    "\n",
    "PyTorchâ€™s Autograd system helps compute these gradients automatically.\n",
    "\n",
    "Without Autograd, we'd have to compute gradients manually, which is inefficient for large models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterxdl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
