{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dc375d2",
   "metadata": {},
   "source": [
    "This is the core engine behind models like BERT and GPT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53411864",
   "metadata": {},
   "source": [
    "What is a Transformer Encoder Block?\n",
    "It’s a modular unit made of:\n",
    "Multi-Head Self-Attention\n",
    "Add & LayerNorm\n",
    "Feedforward Neural Network\n",
    "Add & LayerNorm (again)\n",
    "📦 Think of it as:\n",
    "Input → [Self-Attention + Norm] → [Feedforward + Norm] → Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0417be75",
   "metadata": {},
   "source": [
    " 1. Multi-Head Self-Attention\n",
    "From our previous session:\n",
    "Learns contextual relationships between tokens\n",
    "Uses multiple attention heads to focus on different parts of the sentence\n",
    "Input shape: \n",
    "\n",
    "[batch,seq_len,dmodel ]\n",
    "Output shape: same as input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54048aa0",
   "metadata": {},
   "source": [
    " 2. Add & LayerNorm\n",
    "\\text{LayerNorm}(x + \\text{SelfAttention}(x))\n",
    "]\n",
    "Residual connection helps gradient flow\n",
    "LayerNorm stabilizes training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b00acfc",
   "metadata": {},
   "source": [
    " 3. Position-wise Feedforward Network\n",
    "A simple 2-layer MLP applied to each token:\n",
    "FFN\n",
    "\n",
    "FFN(x)=max(0,xW \n",
    "1\n",
    "​\t\n",
    " +b \n",
    "1\n",
    "​\t\n",
    " )W \n",
    "2\n",
    "​\t\n",
    " +b \n",
    "2\n",
    "​\t\n",
    " \n",
    "Non-linearity (usually ReLU or GELU)\n",
    "Operates independently on each position"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc97611f",
   "metadata": {},
   "source": [
    " 4. Add & LayerNorm Again\n",
    "\\text{LayerNorm}(x + \\text{FFN}(x))]\n",
    "Another residual path\n",
    "Helps model train deeper stacks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ff0649",
   "metadata": {},
   "source": [
    "🔁 This whole block is repeated N times (e.g., 6 in original Transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6581f6b3",
   "metadata": {},
   "source": [
    "Full Flow Diagram (Textual)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3736fd57",
   "metadata": {},
   "source": [
    "[Input + Positional Encoding]\n",
    "           │\n",
    "           ▼\n",
    "┌───────────────────────────────┐\n",
    "│ Multi-Head Self-Attention     │\n",
    "└───────────────────────────────┘\n",
    "           │\n",
    "        (Add + LayerNorm)\n",
    "           ▼\n",
    "┌───────────────────────────────┐\n",
    "│ Feedforward Neural Network    │\n",
    "└───────────────────────────────┘\n",
    "           │\n",
    "        (Add + LayerNorm)\n",
    "           ▼\n",
    "        [Output Representation]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974b7306",
   "metadata": {},
   "source": [
    "| Component       | Purpose                              |\n",
    "| --------------- | ------------------------------------ |\n",
    "| Self-Attention  | Capture relationships between tokens |\n",
    "| Residual + Norm | Stable training & gradient flow      |\n",
    "| Feedforward     | Adds complexity & depth              |\n",
    "| Stackable       | Enables rich, layered understanding  |"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
