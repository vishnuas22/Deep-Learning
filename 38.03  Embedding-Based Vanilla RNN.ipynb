{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cb2a725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/100, Loss: 0.1456\n",
      "Epoch 40/100, Loss: 0.0206\n",
      "Epoch 60/100, Loss: 0.0093\n",
      "Epoch 80/100, Loss: 0.0063\n",
      "Epoch 100/100, Loss: 0.0049\n",
      "\n",
      "ðŸŽ¯ Target Characters   : ['B', 'A', 'A', 'D']\n",
      "ðŸ¤– Predicted Characters: ['B', 'A', 'A', 'D']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Step 1: Define character mappings\n",
    "char_to_idx = {'A': 0, 'B': 1, 'C': 2, 'D': 3}\n",
    "idx_to_char = {i: c for c, i in char_to_idx.items()}\n",
    "\n",
    "input_indices = torch.tensor([2, 1, 0, 0])  # 'C', 'B', 'A', 'A'\n",
    "target_indices = torch.tensor([1, 0, 0, 3]) # 'B', 'A', 'A', 'D'\n",
    "\n",
    "# Step 2: Model definition\n",
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim):\n",
    "        super(CharRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.rnn = nn.RNN(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x = self.embedding(x)            # x: [batch_size, seq_len] -> [batch_size, seq_len, embed_dim]\n",
    "        out, hidden = self.rnn(x, hidden)  # out: [batch_size, seq_len, hidden_dim]\n",
    "        out = self.fc(out)                # out: [batch_size, seq_len, output_dim]\n",
    "        return out, hidden\n",
    "\n",
    "# Step 3: Hyperparameters and setup\n",
    "vocab_size = 4\n",
    "embed_dim = 8\n",
    "hidden_dim = 16\n",
    "output_dim = 4\n",
    "\n",
    "model = CharRNN(vocab_size, embed_dim, hidden_dim, output_dim)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Step 4: Prepare input\n",
    "input_seq = input_indices.unsqueeze(0)     # [1, seq_len]\n",
    "target_seq = target_indices.unsqueeze(0)   # [1, seq_len]\n",
    "\n",
    "# Step 5: Training loop\n",
    "n_epochs = 100\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    hidden = torch.zeros(1, 1, hidden_dim)\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    output, hidden = model(input_seq, hidden)   # output: [1, seq_len, vocab_size]\n",
    "    output = output.squeeze(0)                  # [seq_len, vocab_size]\n",
    "    \n",
    "    loss = loss_fn(output, target_indices)      # [seq_len]\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch}/{n_epochs}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Step 6: Inference\n",
    "with torch.no_grad():\n",
    "    hidden = torch.zeros(1, 1, hidden_dim)\n",
    "    output, hidden = model(input_seq, hidden)\n",
    "    predictions = torch.argmax(output, dim=2).squeeze(0)\n",
    "\n",
    "    predicted_chars = [idx_to_char[i.item()] for i in predictions]\n",
    "    target_chars = [idx_to_char[i.item()] for i in target_indices]\n",
    "\n",
    "    print(\"\\nðŸŽ¯ Target Characters   :\", target_chars)\n",
    "    print(\"ðŸ¤– Predicted Characters:\", predicted_chars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ab0c698",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "perplexity must be less than n_samples",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Reduce dimensions using t-SNE\u001b[39;00m\n\u001b[1;32m      9\u001b[0m tsne \u001b[38;5;241m=\u001b[39m TSNE(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m reduced_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mtsne\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Plot the embeddings\u001b[39;00m\n\u001b[1;32m     13\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n",
      "File \u001b[0;32m/opt/anaconda3/envs/masterxdl/lib/python3.9/site-packages/sklearn/utils/_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    325\u001b[0m         )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/masterxdl/lib/python3.9/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/masterxdl/lib/python3.9/site-packages/sklearn/manifold/_t_sne.py:1177\u001b[0m, in \u001b[0;36mTSNE.fit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1175\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_iter\n\u001b[0;32m-> 1177\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_params_vs_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1178\u001b[0m embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X)\n\u001b[1;32m   1179\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_ \u001b[38;5;241m=\u001b[39m embedding\n",
      "File \u001b[0;32m/opt/anaconda3/envs/masterxdl/lib/python3.9/site-packages/sklearn/manifold/_t_sne.py:862\u001b[0m, in \u001b[0;36mTSNE._check_params_vs_input\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    860\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_check_params_vs_input\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mperplexity \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m--> 862\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mperplexity must be less than n_samples\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: perplexity must be less than n_samples"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "\n",
    "# Extract embeddings from the trained model\n",
    "embeddings = model.embedding.weight.data.numpy()  # shape: [vocab_size, embed_dim]\n",
    "\n",
    "# Reduce dimensions using t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "reduced_embeddings = tsne.fit_transform(embeddings)\n",
    "\n",
    "# Plot the embeddings\n",
    "plt.figure(figsize=(6, 6))\n",
    "for i, label in enumerate(char_to_idx.keys()):\n",
    "    x, y = reduced_embeddings[i]\n",
    "    plt.scatter(x, y)\n",
    "    plt.text(x + 0.01, y + 0.01, label, fontsize=14)\n",
    "\n",
    "plt.title(\"t-SNE Visualization of Character Embeddings\")\n",
    "plt.xlabel(\"Dimension 1\")\n",
    "plt.ylabel(\"Dimension 2\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterxdl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
