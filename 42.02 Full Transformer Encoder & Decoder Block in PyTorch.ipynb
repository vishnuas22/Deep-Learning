{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f10d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        self.d_head = d_model // num_heads\n",
    "\n",
    "        self.qkv_proj = nn.Linear(d_model, 3 * d_model)\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        B, T, D = x.shape\n",
    "\n",
    "        qkv = self.qkv_proj(x)  # Shape: [B, T, 3*D]\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "\n",
    "        # Split into heads\n",
    "        q = q.view(B, T, self.num_heads, self.d_head).transpose(1, 2)\n",
    "        k = k.view(B, T, self.num_heads, self.d_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.num_heads, self.d_head).transpose(1, 2)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        scores = (q @ k.transpose(-2, -1)) / self.d_head**0.5\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_output = attn_weights @ v  # Shape: [B, num_heads, T, d_head]\n",
    "\n",
    "        # Concatenate heads\n",
    "        out = attn_output.transpose(1, 2).contiguous().view(B, T, D)\n",
    "        return self.out_proj(out)\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.dropout(F.relu(self.linear1(x))))\n",
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadSelfAttention(d_model, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        attn_out = self.attn(x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_out))  # Residual + Norm\n",
    "        ff_out = self.ff(x)\n",
    "        x = self.norm2(x + self.dropout(ff_out))    # Residual + Norm\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5149c72f",
   "metadata": {},
   "source": [
    "What‚Äôs Inside"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910377f7",
   "metadata": {},
   "source": [
    "| Module                    | Purpose                                      |\n",
    "| ------------------------- | -------------------------------------------- |\n",
    "| `MultiHeadSelfAttention`  | Implements multi-head self-attention         |\n",
    "| `PositionwiseFeedForward` | Adds depth & complexity to each token        |\n",
    "| `TransformerEncoderBlock` | Combines attention + FFN + residuals & norms |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6186d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Settings\n",
    "batch_size = 2\n",
    "seq_len = 5\n",
    "d_model = 32\n",
    "num_heads = 4\n",
    "d_ff = 64\n",
    "\n",
    "# Dummy input tensor (like output of embedding layer)\n",
    "dummy_input = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# Transformer Encoder Block\n",
    "encoder = TransformerEncoderBlock(d_model=d_model, num_heads=num_heads, d_ff=d_ff)\n",
    "\n",
    "# Forward pass\n",
    "output = encoder(dummy_input)\n",
    "\n",
    "print(\"Input Shape:\", dummy_input.shape)\n",
    "print(\"Output Shape:\", output.shape)\n",
    "print(\"\\nOutput Tensor:\")\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7394c1b",
   "metadata": {},
   "source": [
    "Output Interpretation\n",
    "Shape remains the same: (batch_size, seq_len, d_model)\n",
    "But the values are transformed via:\n",
    "Self-attention to capture relationships across tokens.\n",
    "Feedforward to add depth.\n",
    "LayerNorm + Residuals for training stability.\n",
    "This step lets you trace how input embeddings evolve through the encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0887a42e",
   "metadata": {},
   "source": [
    "Step 3: Stack Multiple Transformer Encoder Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca19424",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, d_ff):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderBlock(d_model, num_heads, d_ff)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return self.norm(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3517391",
   "metadata": {},
   "source": [
    " Explanation:\n",
    "nn.ModuleList: allows stacking submodules (like encoder blocks).\n",
    "Each layer passes output to the next ‚Üí deep sequence modeling.\n",
    "Final LayerNorm stabilizes the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c409de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "num_layers = 4  # Stack 4 encoder layers\n",
    "encoder_stack = TransformerEncoder(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    d_ff=d_ff\n",
    ")\n",
    "\n",
    "# Pass dummy input through stacked encoder\n",
    "stacked_output = encoder_stack(dummy_input)\n",
    "\n",
    "print(\"Final Stacked Output Shape:\", stacked_output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efeaf86",
   "metadata": {},
   "source": [
    "Next: Add Positional Encoding\n",
    "\n",
    "‚ö†Ô∏è Transformers have no recurrence or convolution. They need positional encodings to understand sequence order.\n",
    "This step is crucial before using real data, especially for tasks like classification, translation, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0aea9f9",
   "metadata": {},
   "source": [
    " Why Positional Encoding?\n",
    "The attention mechanism treats inputs like a set, not a sequence. So:\n",
    "‚ÄúI love AI‚Äù ‚Üí same as ‚Üí ‚ÄúAI love I‚Äù\n",
    "Without position, order is lost\n",
    "Positional encodings inject sequence order information into the input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ec1ec7",
   "metadata": {},
   "source": [
    " Sinusoidal Positional Encoding (Vaswani et al.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cbb381",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # even\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # odd\n",
    "\n",
    "        pe = pe.unsqueeze(0)  # Shape: (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dade22",
   "metadata": {},
   "source": [
    "Use it in the Encoder Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1f1522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize positional encoding\n",
    "pos_enc = PositionalEncoding(d_model)\n",
    "\n",
    "# Add positional info to dummy input\n",
    "x_with_pos = pos_enc(dummy_input)\n",
    "\n",
    "# Pass through stacked encoder\n",
    "final_output = encoder_stack(x_with_pos)\n",
    "print(\"Output shape with positional encoding:\", final_output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1b3618",
   "metadata": {},
   "source": [
    "Step 5: Decoder Block + Cross Attention\n",
    "\n",
    "üî• To complete the full Transformer architecture, especially for sequence-to-sequence tasks like machine translation, text summarization, or code generation.\n",
    "‚úÖ Why This Now?\n",
    "We‚Äôve:\n",
    "‚úÖ Built the full encoder with attention + position\n",
    "‚úÖ Studied attention in isolation\n",
    "üö´ Not yet seen how Decoder uses both self-attention and cross-attention with encoder output.\n",
    "üéØ Learning Goal:\n",
    "Understand and implement a Transformer Decoder Block with:\n",
    "Masked self-attention (so it doesn't peek into the future)\n",
    "Cross-attention (attends to encoder outputs)\n",
    "Feedforward + residuals + layer norms\n",
    "Once done, you‚Äôll understand 99% of the architecture behind models like:\n",
    "GPT (decoder-only)\n",
    "BERT (encoder-only)\n",
    "T5, BART, etc. (encoder-decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9df9cb",
   "metadata": {},
   "source": [
    "Transformer Decoder Block: Core Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52000a3a",
   "metadata": {},
   "source": [
    "Input ‚Üí Masked Self-Attention\n",
    "      ‚Üí Add & Norm\n",
    "      ‚Üí Cross-Attention (Encoder Output)\n",
    "      ‚Üí Add & Norm\n",
    "      ‚Üí Feed Forward Network (FFN)\n",
    "      ‚Üí Add & Norm ‚Üí Output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c618a66",
   "metadata": {},
   "source": [
    "üîí 1. Masked Multi-Head Self-Attention\n",
    "Prevents future tokens from being seen.\n",
    "Like: ‚ÄúI am [MASK]‚Äù ‚Üí can‚Äôt attend to words beyond [MASK].\n",
    "Uses a causal mask (triangular matrix) to block right-side tokens.\n",
    "üîÅ 2. Cross-Attention\n",
    "Allows decoder to attend to encoder‚Äôs outputs.\n",
    "Decoder Query (Q) attends to encoder Key/Value (K, V).\n",
    "Critical for seq2seq tasks like translation.\n",
    "‚öôÔ∏è 3. Feedforward + Add & Norm\n",
    "Just like encoder: 2-layer FFN + residuals + layer norm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8238578",
   "metadata": {},
   "source": [
    "+-------------------------+\n",
    "|      Masked Self-Attn   | ‚Üê attends to past decoder tokens only\n",
    "+-------------------------+\n",
    "|      Add & LayerNorm    |\n",
    "+-------------------------+\n",
    "|      Cross-Attention    | ‚Üê attends to encoder output\n",
    "+-------------------------+\n",
    "|      Add & LayerNorm    |\n",
    "+-------------------------+\n",
    "| Feed Forward (2-layer)  |\n",
    "+-------------------------+\n",
    "|      Add & LayerNorm    |\n",
    "+-------------------------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ae3532",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TransformerDecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n",
    "        self.cross_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n",
    "\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.norm3 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None):\n",
    "        # 1. Masked Self-Attention (causal)\n",
    "        tgt2, _ = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask)\n",
    "        tgt = self.norm1(tgt + self.dropout1(tgt2))\n",
    "\n",
    "        # 2. Cross-Attention (encoder output)\n",
    "        tgt2, _ = self.cross_attn(tgt, memory, memory, attn_mask=memory_mask)\n",
    "        tgt = self.norm2(tgt + self.dropout2(tgt2))\n",
    "\n",
    "        # 3. Feed Forward\n",
    "        tgt2 = self.ffn(tgt)\n",
    "        tgt = self.norm3(tgt + self.dropout3(tgt2))\n",
    "\n",
    "        return tgt\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
