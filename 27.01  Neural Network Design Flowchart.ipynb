{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "📌 START\n",
    "   │\n",
    "   ├──▶ 🧩 1. Understand the Problem Type\n",
    "   │       ├── Classification?\n",
    "   │       │     ├── Binary → Use Sigmoid + Binary Crossentropy\n",
    "   │       │     └── Multi-class → Use Softmax + Categorical Crossentropy\n",
    "   │       │\n",
    "   │       └── Regression?\n",
    "   │             └── Use Linear Output + MSE / MAE / Huber\n",
    "   │\n",
    "   ├──▶ 🧠 2. Input Shape and Preprocessing\n",
    "   │       ├── Number of Features?\n",
    "   │       ├── Normalization/Standardization needed?\n",
    "   │       └── Reshape input if needed (especially for images, sequences)\n",
    "   │\n",
    "   ├──▶ 🔧 3. Choose Network Type\n",
    "   │       ├── Tabular → Use MLP (Dense NN)\n",
    "   │       ├── Images → Use CNN\n",
    "   │       ├── Text/Sequences → Use RNN / LSTM / GRU / Transformers\n",
    "   │       └── Time Series → RNN/GRU + Sequence handling\n",
    "   │\n",
    "   ├──▶ 🧱 4. Define Number of Layers & Neurons\n",
    "   │       ├── Shallow Data → Fewer Layers\n",
    "   │       ├── Complex Patterns → More Layers (Deep Learning)\n",
    "   │       ├── Start with: [Input] → [Hidden Layer 1: 4~64 neurons] → ... → [Output]\n",
    "   │       └── Rule of thumb: decrease neurons layer by layer or keep balanced\n",
    "   │\n",
    "   ├──▶ 🔋 5. Choose Activation Functions\n",
    "   │       ├── Hidden Layers\n",
    "   │       │     ├── ReLU (default)\n",
    "   │       │     ├── Leaky ReLU / ELU → If ReLU dying neurons\n",
    "   │       │     └── Tanh → If data is centered around 0\n",
    "   │       └── Output Layer\n",
    "   │             ├── Sigmoid → Binary classification\n",
    "   │             ├── Softmax → Multi-class classification\n",
    "   │             └── Linear → Regression\n",
    "   │\n",
    "   ├──▶ 🧮 6. Choose Loss Function\n",
    "   │       ├── Match with output activation\n",
    "   │       │     ├── Sigmoid → Binary Crossentropy\n",
    "   │       │     ├── Softmax → Categorical Crossentropy\n",
    "   │       │     └── Linear → MSE / MAE\n",
    "   │\n",
    "   ├──▶ ⚙️ 7. Optimizer & Learning Rate\n",
    "   │       ├── Adam (Most used)\n",
    "   │       ├── SGD → When tuning from scratch or prefer stability\n",
    "   │       └── LR Scheduling → Reduce LR on plateau or exponential decay\n",
    "   │\n",
    "   ├──▶ 🧪 8. Training Strategy\n",
    "   │       ├── Use Validation Set\n",
    "   │       ├── Early Stopping\n",
    "   │       ├── Dropout / BatchNorm if needed\n",
    "   │       └── Track Metrics (accuracy, loss, F1, etc.)\n",
    "   │\n",
    "   ├──▶ 🧠 9. Testing & Generalization\n",
    "   │       ├── Evaluate on Test Set\n",
    "   │       └── Avoid overfitting → Regularization, dropout, early stop\n",
    "   │\n",
    "   └──▶ 🚀 10. Fine-Tune & Optimize\n",
    "           ├── Tune hidden layer sizes\n",
    "           ├── Try different activations/loss combos\n",
    "           ├── Try batch sizes, LR, optimizer variations\n",
    "           └── Profile training speed and performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "START\n",
    "  |\n",
    "  |--> What type of problem are you solving?\n",
    "        |\n",
    "        |--> 🧠 Classification?\n",
    "        |        |\n",
    "        |        |--> Binary? (e.g., cat vs dog)\n",
    "        |        |       |\n",
    "        |        |       |--> Output Layer: 1 neuron\n",
    "        |        |       |--> Output Activation: Sigmoid\n",
    "        |        |       |--> Loss Function: BCELoss / BCEWithLogitsLoss (if no sigmoid)\n",
    "        |        |       |\n",
    "        |        |       |--> Hidden Layers: ReLU / LeakyReLU\n",
    "        |\n",
    "        |        |--> Multi-class? (e.g., dog, cat, panda)\n",
    "        |        |       |\n",
    "        |        |       |--> Output Layer: n neurons (n = num classes)\n",
    "        |        |       |--> Output Activation: None (logits)\n",
    "        |        |       |--> Loss Function: CrossEntropyLoss (handles logits + softmax)\n",
    "        |        |       |\n",
    "        |        |       |--> Hidden Layers: ReLU / Swish / GELU\n",
    "        |\n",
    "        |        |--> Multi-label? (e.g., image with cat & dog)\n",
    "        |                |\n",
    "        |                |--> Output Layer: n neurons (n = labels)\n",
    "        |                |--> Output Activation: Sigmoid\n",
    "        |                |--> Loss Function: BCEWithLogitsLoss\n",
    "        |                |\n",
    "        |                |--> Hidden Layers: ReLU / ELU\n",
    "        |\n",
    "        |\n",
    "        |--> 📏 Regression? (e.g., predict house price)\n",
    "        |        |\n",
    "        |        |--> Output Layer: 1 neuron (or more for multi-output)\n",
    "        |        |--> Output Activation: None\n",
    "        |        |--> Loss Function: MSELoss / MAELoss / HuberLoss\n",
    "        |        |\n",
    "        |        |--> Hidden Layers: ReLU / LeakyReLU / Swish\n",
    "        |\n",
    "        |\n",
    "        |--> 🎯 Embedding / Feature Learning?\n",
    "        |        |\n",
    "        |        |--> Output Layer: n-d vector\n",
    "        |        |--> Activation: Often None\n",
    "        |        |--> Loss Function: TripletLoss / ContrastiveLoss / Custom\n",
    "        |\n",
    "        |        |--> Hidden Layers: Tanh / ReLU / GELU\n",
    "\n",
    "  |\n",
    "  |--> Choose Number of Hidden Layers\n",
    "        |\n",
    "        |--> Simple data? 1–2 layers\n",
    "        |--> Tabular data? 2–5 layers\n",
    "        |--> Complex data (images, text)? 5–50+ layers (CNNs, RNNs, Transformers)\n",
    "  |\n",
    "  |--> Choose Hidden Neurons (per layer)\n",
    "        |\n",
    "        |--> Start big, shrink down (e.g., 64 → 32 → 16)\n",
    "        |--> Use trial & error or hyperparameter tuning\n",
    "  |\n",
    "  |--> Choose Hidden Activations\n",
    "        |\n",
    "        |--> Default: ReLU\n",
    "        |--> Dead Neurons? → LeakyReLU / ELU\n",
    "        |--> Smoother learning? → Swish / GELU\n",
    "        |--> Avoid Sigmoid/Tanh in deep networks\n",
    "  |\n",
    "  |--> Optional Add-ons:\n",
    "        |\n",
    "        |--> BatchNorm → More stable training\n",
    "        |--> Dropout → Prevent overfitting\n",
    "        |--> Residuals / Skip Connections → For very deep nets\n",
    "  |\n",
    "  |--> Done! Train your model 🚀\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
