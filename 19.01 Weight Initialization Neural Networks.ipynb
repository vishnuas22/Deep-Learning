{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn as nn \n",
    "\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Define a Simple Neural Network\n",
    "\n",
    "We'll use a 2-layer neural network for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple neural network\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(2,4) # Input layer (2 neurons) â†’ Hidden layer (4 neurons)\n",
    "\n",
    "        self.fc2 = nn.Linear(4,1) # Hidden layer (4 neurons) â†’ Output layer (1 neuron)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        x = F.relu(self.fc1(x)) # ReLU activation for hidden layer\n",
    "\n",
    "        x = torch.sigmoid(self.fc2(x)) # Sigmoid activation for output\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "\n",
    "model = SimpleNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Apply Different Weight Initializations\n",
    "\n",
    "We will define functions for zero, Xavier, He, and random initialization and apply them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Zero Initialization (BAD ðŸš«)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleNN(\n",
       "  (fc1): Linear(in_features=2, out_features=4, bias=True)\n",
       "  (fc2): Linear(in_features=4, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def zero_init(m):\n",
    "\n",
    "    if isinstance(m, nn.Linear):\n",
    "\n",
    "        nn.init.constant_(m.weight,0)\n",
    "\n",
    "        nn.init.constant_(m.bias,0)\n",
    "\n",
    "# Apply Zero Initialization\n",
    "\n",
    "model.apply(zero_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Random Initialization (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleNN(\n",
       "  (fc1): Linear(in_features=2, out_features=4, bias=True)\n",
       "  (fc2): Linear(in_features=4, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def random_init(m):\n",
    "\n",
    "    if isinstance(m, nn.Linear):\n",
    "\n",
    "        nn.init.uniform_(m.weight, -0.1,0.1) # Small random values\n",
    "\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "# Apply Random Initialization\n",
    "\n",
    "model.apply(random_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleNN(\n",
       "  (fc1): Linear(in_features=2, out_features=4, bias=True)\n",
       "  (fc2): Linear(in_features=4, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def xavier_init(m):\n",
    "\n",
    "    if isinstance(m, nn.Linear):\n",
    "\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "        nn.init.constant_(m.weight, 0)\n",
    "\n",
    "\n",
    "# Apply Xavier Initialization\n",
    "\n",
    "model.apply(xavier_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. He (Kaiming) Initialization (Best for ReLU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleNN(\n",
       "  (fc1): Linear(in_features=2, out_features=4, bias=True)\n",
       "  (fc2): Linear(in_features=4, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def he_init(m):\n",
    "\n",
    "    if isinstance(m, nn.Linear):\n",
    "\n",
    "        nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n",
    "\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "# Apply He Initialization\n",
    "\n",
    "model.apply(he_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Verify Initialization\n",
    "\n",
    "After applying an initialization method, we can inspect the initialized weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.weight : tensor([[ 1.1865,  0.6319],\n",
      "        [ 0.4075, -0.7439],\n",
      "        [ 1.2187,  0.6133],\n",
      "        [ 1.6621, -0.8599]]) \n",
      "\n",
      "fc1.bias : tensor([0., 0., 0., 0.]) \n",
      "\n",
      "fc2.weight : tensor([[0.7613, 0.5820, 1.0290, 0.5916]]) \n",
      "\n",
      "fc2.bias : tensor([0.]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "\n",
    "    print(f'{name} : {param.data} \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which Initialization is Best?\n",
    "\n",
    "Zero Initialization: Causes neurons to learn the same features (not useful).\n",
    "\n",
    "Random Initialization: Works but can be unstable.\n",
    "\n",
    "Xavier Initialization: Good for sigmoid/tanh activations.\n",
    "\n",
    "He Initialization: Best for ReLU networks (recommended for deep networks)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterxdl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
