{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "250d5107",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f55f589b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Tokenize corpus into words\n",
    "\n",
    "corpus = \"deep learning is fun and deep learning is powerful\"\n",
    "\n",
    "words = corpus.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "820cf31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['and', 'deep', 'fun', 'is', 'learning', 'powerful']\n",
      "word_to_idx: {'and': 0, 'deep': 1, 'fun': 2, 'is': 3, 'learning': 4, 'powerful': 5}\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Build vocabulary\n",
    "\n",
    "vocab = sorted(set(words))\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "\n",
    "word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
    "\n",
    "idx_to_word = {i: word for word, i in word_to_idx.items()}\n",
    "\n",
    "print(\"Vocabulary:\", vocab)\n",
    "\n",
    "print(\"word_to_idx:\", word_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24e8c2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sentences = [\n",
    "    ['deep', 'learning', 'is', 'fun'],\n",
    "    ['learning', 'is', 'powerful'],\n",
    "    ['deep', 'learning', 'is', 'powerful'],\n",
    "    ['learning', 'is', 'fun', 'and', 'powerful']\n",
    "]\n",
    "\n",
    "seq_length = 3  # words per input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6405c316",
   "metadata": {},
   "source": [
    "Convert to Tensors\n",
    "We need to:\n",
    "Map words to indices using word_to_idx\n",
    "Slice sequences into input–target pairs\n",
    "Store them as tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90d7b46e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Input: tensor([1, 4, 3])\n",
      "Sample target: tensor(2)\n"
     ]
    }
   ],
   "source": [
    "input_seqs = []\n",
    "\n",
    "target_words = []\n",
    "\n",
    "\n",
    "for sentance in training_sentences:\n",
    "\n",
    "    if len(sentance) <= seq_length:\n",
    "\n",
    "        continue\n",
    "\n",
    "    for i in range(len(sentance)- seq_length):\n",
    "\n",
    "        input_seq = sentance[i: i + seq_length]\n",
    "\n",
    "        target_word = sentance[i + seq_length]\n",
    "\n",
    "\n",
    "        input_tensor = torch.tensor([word_to_idx[word] for word in input_seq], dtype=torch.long)\n",
    "\n",
    "        target_tensor = torch.tensor(word_to_idx[target_word], dtype=torch.long)\n",
    "\n",
    "\n",
    "        input_seqs.append(input_tensor)\n",
    "\n",
    "        target_words.append(target_tensor)\n",
    "\n",
    "\n",
    "print('Sample Input:',input_seqs[0])\n",
    "\n",
    "print('Sample target:',target_words[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767789fa",
   "metadata": {},
   "source": [
    "Step 4: Build the Word-Level RNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40bd60a",
   "metadata": {},
   "source": [
    "This is just like the character-level RNN we trained earlier—but now it processes sequences of word indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "365701a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordRNN(nn.Module):\n",
    "\n",
    "    def __init__(self,vocab_size, embedding_dim, hidden_dim):\n",
    "        super(WordRNN, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "\n",
    "        self.fc = nn. Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "\n",
    "        embedded = self.embedding(x)          # [batch_size, seq_len, embedding_dim]\n",
    "\n",
    "\n",
    "        out, hidden = self.rnn(embedded, hidden)\n",
    "\n",
    "        out = self.fc(out[:, -1, :])          # Only use the output of the last time step\n",
    "\n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fab24f",
   "metadata": {},
   "source": [
    "nn.Embedding maps words to dense vectors\n",
    "\n",
    "RNN processes word-by-word\n",
    "\n",
    "Final output is fed into a linear layer to predict the next word in the sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d967b5cc",
   "metadata": {},
   "source": [
    "🔧 Define Model + Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e63b3d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 10\n",
    "\n",
    "hidden_dim = 16\n",
    "\n",
    "vocab_size = 16\n",
    "\n",
    "\n",
    "model = WordRNN(vocab_size=len(vocab), embedding_dim=embedding_dim, hidden_dim=hidden_dim)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8ebc1a",
   "metadata": {},
   "source": [
    "⚙️ Training Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69300070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/300, Loss: 0.0246\n",
      "Sample Predictions:\n",
      "['deep', 'learning', 'is']  → powerful\n",
      "['deep', 'learning', 'is']  → powerful\n",
      "['learning', 'is', 'fun']  → and\n",
      "['is', 'fun', 'and']  → powerful\n",
      "Epoch 100/300, Loss: 0.0120\n",
      "Sample Predictions:\n",
      "['deep', 'learning', 'is']  → powerful\n",
      "['deep', 'learning', 'is']  → powerful\n",
      "['learning', 'is', 'fun']  → and\n",
      "['is', 'fun', 'and']  → powerful\n",
      "Epoch 150/300, Loss: 0.0079\n",
      "Sample Predictions:\n",
      "['deep', 'learning', 'is']  → powerful\n",
      "['deep', 'learning', 'is']  → powerful\n",
      "['learning', 'is', 'fun']  → and\n",
      "['is', 'fun', 'and']  → powerful\n",
      "Epoch 200/300, Loss: 0.0059\n",
      "Sample Predictions:\n",
      "['deep', 'learning', 'is']  → powerful\n",
      "['deep', 'learning', 'is']  → powerful\n",
      "['learning', 'is', 'fun']  → and\n",
      "['is', 'fun', 'and']  → powerful\n",
      "Epoch 250/300, Loss: 0.0047\n",
      "Sample Predictions:\n",
      "['deep', 'learning', 'is']  → powerful\n",
      "['deep', 'learning', 'is']  → powerful\n",
      "['learning', 'is', 'fun']  → and\n",
      "['is', 'fun', 'and']  → powerful\n",
      "Epoch 300/300, Loss: 0.0039\n",
      "Sample Predictions:\n",
      "['deep', 'learning', 'is']  → powerful\n",
      "['deep', 'learning', 'is']  → powerful\n",
      "['learning', 'is', 'fun']  → and\n",
      "['is', 'fun', 'and']  → powerful\n"
     ]
    }
   ],
   "source": [
    "# Training parameters\n",
    "\n",
    "num_epochs = 300\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "\n",
    "model = WordRNN(vocab_size, embedding_dim, hidden_dim)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "\n",
    "    total_loss = 0.0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for input_tensor,target_tensor in zip(input_seqs, target_words):\n",
    "\n",
    "        hidden = torch.zeros(1,1,hidden_dim) # <--- Init hidden state each time\n",
    "\n",
    "        input_tensor = input_tensor.unsqueeze(0)   # [1, seq_len]\n",
    "\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output, hidden = model(input_tensor, hidden)\n",
    "\n",
    "\n",
    "        loss = loss_fn(output, target_tensor.unsqueeze(0))\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "\n",
    "\n",
    "    if epoch % 50 == 0:\n",
    "\n",
    "        print(f\"Epoch {epoch}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "\n",
    "        # Show predictions for all samples\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        print(\"Sample Predictions:\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for input_tensor in input_seqs:\n",
    "\n",
    "                hidden = torch.zeros(1,1,hidden_dim)\n",
    "\n",
    "                input_tensor = input_tensor.unsqueeze(0)\n",
    "\n",
    "                output, hidden = model(input_tensor, hidden)\n",
    "\n",
    "                pred_idx = torch.argmax(output, dim=1).item()\n",
    "\n",
    "                predicted_word = idx_to_word[pred_idx]\n",
    "\n",
    "                input_words = [idx_to_word[idx.item()] for idx in input_tensor.squeeze()]\n",
    "\n",
    "                print(f'{input_words}  → {predicted_word}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterxdl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
