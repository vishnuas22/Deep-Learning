{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ“Œ START\n",
    "   â”‚\n",
    "   â”œâ”€â”€â–¶ ðŸ§© 1. Understand the Problem Type\n",
    "   â”‚       â”œâ”€â”€ Classification?\n",
    "   â”‚       â”‚     â”œâ”€â”€ Binary â†’ Use Sigmoid + Binary Crossentropy\n",
    "   â”‚       â”‚     â””â”€â”€ Multi-class â†’ Use Softmax + Categorical Crossentropy\n",
    "   â”‚       â”‚\n",
    "   â”‚       â””â”€â”€ Regression?\n",
    "   â”‚             â””â”€â”€ Use Linear Output + MSE / MAE / Huber\n",
    "   â”‚\n",
    "   â”œâ”€â”€â–¶ ðŸ§  2. Input Shape and Preprocessing\n",
    "   â”‚       â”œâ”€â”€ Number of Features?\n",
    "   â”‚       â”œâ”€â”€ Normalization/Standardization needed?\n",
    "   â”‚       â””â”€â”€ Reshape input if needed (especially for images, sequences)\n",
    "   â”‚\n",
    "   â”œâ”€â”€â–¶ ðŸ”§ 3. Choose Network Type\n",
    "   â”‚       â”œâ”€â”€ Tabular â†’ Use MLP (Dense NN)\n",
    "   â”‚       â”œâ”€â”€ Images â†’ Use CNN\n",
    "   â”‚       â”œâ”€â”€ Text/Sequences â†’ Use RNN / LSTM / GRU / Transformers\n",
    "   â”‚       â””â”€â”€ Time Series â†’ RNN/GRU + Sequence handling\n",
    "   â”‚\n",
    "   â”œâ”€â”€â–¶ ðŸ§± 4. Define Number of Layers & Neurons\n",
    "   â”‚       â”œâ”€â”€ Shallow Data â†’ Fewer Layers\n",
    "   â”‚       â”œâ”€â”€ Complex Patterns â†’ More Layers (Deep Learning)\n",
    "   â”‚       â”œâ”€â”€ Start with: [Input] â†’ [Hidden Layer 1: 4~64 neurons] â†’ ... â†’ [Output]\n",
    "   â”‚       â””â”€â”€ Rule of thumb: decrease neurons layer by layer or keep balanced\n",
    "   â”‚\n",
    "   â”œâ”€â”€â–¶ ðŸ”‹ 5. Choose Activation Functions\n",
    "   â”‚       â”œâ”€â”€ Hidden Layers\n",
    "   â”‚       â”‚     â”œâ”€â”€ ReLU (default)\n",
    "   â”‚       â”‚     â”œâ”€â”€ Leaky ReLU / ELU â†’ If ReLU dying neurons\n",
    "   â”‚       â”‚     â””â”€â”€ Tanh â†’ If data is centered around 0\n",
    "   â”‚       â””â”€â”€ Output Layer\n",
    "   â”‚             â”œâ”€â”€ Sigmoid â†’ Binary classification\n",
    "   â”‚             â”œâ”€â”€ Softmax â†’ Multi-class classification\n",
    "   â”‚             â””â”€â”€ Linear â†’ Regression\n",
    "   â”‚\n",
    "   â”œâ”€â”€â–¶ ðŸ§® 6. Choose Loss Function\n",
    "   â”‚       â”œâ”€â”€ Match with output activation\n",
    "   â”‚       â”‚     â”œâ”€â”€ Sigmoid â†’ Binary Crossentropy\n",
    "   â”‚       â”‚     â”œâ”€â”€ Softmax â†’ Categorical Crossentropy\n",
    "   â”‚       â”‚     â””â”€â”€ Linear â†’ MSE / MAE\n",
    "   â”‚\n",
    "   â”œâ”€â”€â–¶ âš™ï¸ 7. Optimizer & Learning Rate\n",
    "   â”‚       â”œâ”€â”€ Adam (Most used)\n",
    "   â”‚       â”œâ”€â”€ SGD â†’ When tuning from scratch or prefer stability\n",
    "   â”‚       â””â”€â”€ LR Scheduling â†’ Reduce LR on plateau or exponential decay\n",
    "   â”‚\n",
    "   â”œâ”€â”€â–¶ ðŸ§ª 8. Training Strategy\n",
    "   â”‚       â”œâ”€â”€ Use Validation Set\n",
    "   â”‚       â”œâ”€â”€ Early Stopping\n",
    "   â”‚       â”œâ”€â”€ Dropout / BatchNorm if needed\n",
    "   â”‚       â””â”€â”€ Track Metrics (accuracy, loss, F1, etc.)\n",
    "   â”‚\n",
    "   â”œâ”€â”€â–¶ ðŸ§  9. Testing & Generalization\n",
    "   â”‚       â”œâ”€â”€ Evaluate on Test Set\n",
    "   â”‚       â””â”€â”€ Avoid overfitting â†’ Regularization, dropout, early stop\n",
    "   â”‚\n",
    "   â””â”€â”€â–¶ ðŸš€ 10. Fine-Tune & Optimize\n",
    "           â”œâ”€â”€ Tune hidden layer sizes\n",
    "           â”œâ”€â”€ Try different activations/loss combos\n",
    "           â”œâ”€â”€ Try batch sizes, LR, optimizer variations\n",
    "           â””â”€â”€ Profile training speed and performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "START\n",
    "  |\n",
    "  |--> What type of problem are you solving?\n",
    "        |\n",
    "        |--> ðŸ§  Classification?\n",
    "        |        |\n",
    "        |        |--> Binary? (e.g., cat vs dog)\n",
    "        |        |       |\n",
    "        |        |       |--> Output Layer: 1 neuron\n",
    "        |        |       |--> Output Activation: Sigmoid\n",
    "        |        |       |--> Loss Function: BCELoss / BCEWithLogitsLoss (if no sigmoid)\n",
    "        |        |       |\n",
    "        |        |       |--> Hidden Layers: ReLU / LeakyReLU\n",
    "        |\n",
    "        |        |--> Multi-class? (e.g., dog, cat, panda)\n",
    "        |        |       |\n",
    "        |        |       |--> Output Layer: n neurons (n = num classes)\n",
    "        |        |       |--> Output Activation: None (logits)\n",
    "        |        |       |--> Loss Function: CrossEntropyLoss (handles logits + softmax)\n",
    "        |        |       |\n",
    "        |        |       |--> Hidden Layers: ReLU / Swish / GELU\n",
    "        |\n",
    "        |        |--> Multi-label? (e.g., image with cat & dog)\n",
    "        |                |\n",
    "        |                |--> Output Layer: n neurons (n = labels)\n",
    "        |                |--> Output Activation: Sigmoid\n",
    "        |                |--> Loss Function: BCEWithLogitsLoss\n",
    "        |                |\n",
    "        |                |--> Hidden Layers: ReLU / ELU\n",
    "        |\n",
    "        |\n",
    "        |--> ðŸ“ Regression? (e.g., predict house price)\n",
    "        |        |\n",
    "        |        |--> Output Layer: 1 neuron (or more for multi-output)\n",
    "        |        |--> Output Activation: None\n",
    "        |        |--> Loss Function: MSELoss / MAELoss / HuberLoss\n",
    "        |        |\n",
    "        |        |--> Hidden Layers: ReLU / LeakyReLU / Swish\n",
    "        |\n",
    "        |\n",
    "        |--> ðŸŽ¯ Embedding / Feature Learning?\n",
    "        |        |\n",
    "        |        |--> Output Layer: n-d vector\n",
    "        |        |--> Activation: Often None\n",
    "        |        |--> Loss Function: TripletLoss / ContrastiveLoss / Custom\n",
    "        |\n",
    "        |        |--> Hidden Layers: Tanh / ReLU / GELU\n",
    "\n",
    "  |\n",
    "  |--> Choose Number of Hidden Layers\n",
    "        |\n",
    "        |--> Simple data? 1â€“2 layers\n",
    "        |--> Tabular data? 2â€“5 layers\n",
    "        |--> Complex data (images, text)? 5â€“50+ layers (CNNs, RNNs, Transformers)\n",
    "  |\n",
    "  |--> Choose Hidden Neurons (per layer)\n",
    "        |\n",
    "        |--> Start big, shrink down (e.g., 64 â†’ 32 â†’ 16)\n",
    "        |--> Use trial & error or hyperparameter tuning\n",
    "  |\n",
    "  |--> Choose Hidden Activations\n",
    "        |\n",
    "        |--> Default: ReLU\n",
    "        |--> Dead Neurons? â†’ LeakyReLU / ELU\n",
    "        |--> Smoother learning? â†’ Swish / GELU\n",
    "        |--> Avoid Sigmoid/Tanh in deep networks\n",
    "  |\n",
    "  |--> Optional Add-ons:\n",
    "        |\n",
    "        |--> BatchNorm â†’ More stable training\n",
    "        |--> Dropout â†’ Prevent overfitting\n",
    "        |--> Residuals / Skip Connections â†’ For very deep nets\n",
    "  |\n",
    "  |--> Done! Train your model ðŸš€\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
